{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/mrzaizai2k/code_Bao/ViT5/examples\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "try:\n",
        "    print(file_path)\n",
        "except:\n",
        "    file_path = os.path.abspath('')\n",
        "    os.chdir(os.path.dirname(file_path))\n",
        "    print(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3_GCCIaj7ulj"
      },
      "outputs": [],
      "source": [
        "# !wget 'https://github.com/ThanhChinhBK/vietnews/archive/master.zip'\n",
        "# !unzip 'master.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TZYjioRkKviO"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "import concurrent.futures\n",
        "from datasets import *\n",
        "import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "\n",
        "# from vncorenlp import VnCoreNLP\n",
        "import torch \n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Gx4Vg4cbEUJ_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR = 'vietnamese_t5_summary_model_falcon'\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/mrzaizai2k/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IteMtlc58-y"
      },
      "source": [
        "## Processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "UVkc5HmK6Bdd"
      },
      "outputs": [],
      "source": [
        "def listPaths(path):\n",
        "  pathfiles = list()\n",
        "  for pathfile in glob.glob(path):\n",
        "    pathfiles.append(pathfile)\n",
        "  return pathfiles\n",
        "\n",
        "train_paths = listPaths('data/vietnews-master/data/train_tokenized/*')\n",
        "val_paths = listPaths('data/vietnews-master/data/val_tokenized/*')\n",
        "test_paths = listPaths('data/vietnews-master/data/test_tokenized/*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YZ8pgIYN7zSW"
      },
      "outputs": [],
      "source": [
        "def read_content(pathfile):\n",
        "    \"\"\"\n",
        "    Input: Path of txt file\n",
        "    Output: A dictionary has keys 'original' and 'summary'\n",
        "    \"\"\"\n",
        "    with open(pathfile) as f:\n",
        "      rows  = f.readlines()\n",
        "      original = ' '.join(''.join(rows[4:]).split('\\n'))\n",
        "      summary = ' '.join(rows[2].split('\\n'))\n",
        "            \n",
        "    return {'file' : pathfile,\n",
        "              'original': original, \n",
        "              'summary': summary}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_716GF2iDTcD",
        "outputId": "07268442-c69d-4818-a69a-2f9200044eed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'file': 'data/vietnews-master/data/train_tokenized/006157.txt.seg',\n",
              " 'original': 'Tập 4 Thần_tượng Âm_nhạc nhí - Vietnam_Idol_Kids 2017 lên sóng tối 2/6 với màn tranh tài của top 8 thí_sinh nữ để lựa_chọn ra 5 tấm vé vào tiếp vòng trong . Trong đó , 3 giám_khảo Isaac - Văn_Mai_Hương - Bích_Phương được quyền lựa_chọn 3 thí_sinh . 2 thí_sinh còn lại sẽ do các giám_khảo khách mời quyết_định . Nữ ca_sĩ Văn_Mai_Hương xúc_động : “ Cô có may_mắn năm nay ngồi ở vị_trí ban giám_khảo , may_mắn hơn là được gặp con . Mỗi khi gặp Hiền , cô tự thấy bản_thân cô rất kém , bởi có lúc cô không trân_trọng cũng như không tin vào bản_thân ... Cảm_ơn con , bởi đôi_khi có những cái cô không bằng con được , đó là sự lạc_quan . Và cô tin , còn rất nhiều người phải học đức_tính lạc_quan này của Hiền . Con hát rất là hay ” . Đồng_tình với ý_kiến của đồng_nghiệp , Bích_Phương cũng xúc_động chia_sẻ : “ Giọt nước_mắt dành cho con là sự khâm_phục chứ không phải là thương_cảm . Cô chưa bao_giờ thấy con buồn . Từ lúc xuất_hiện , lúc_nào con cũng cười thôi . Cô nghĩ là ngoài tinh_thần lạc_quan của Hiền làm cho mọi người phải khâm_phục nên mọi người mới khóc … Bên cạnh đó , con còn có giọng hát rất hay . Con rất xứng_đáng được sâu vào vòng trong ” . Hà_Linh  ',\n",
              " 'summary': 'Trên sân_khấu Vietnam_Idol_Kids 2017 , cô_bé khiếm_thị Minh_Hiền khiến giám_khảo và khán_giả lặng người khi tiết_lộ ước_mơ của bản_thân . '}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "read_content(train_paths[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Wm5kLJD_840E"
      },
      "outputs": [],
      "source": [
        "def get_dataframe(pathfiles):\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "      data = executor.map(read_content, pathfiles)\n",
        "    \n",
        "    # Make blank dataframe\n",
        "    data_df = list()\n",
        "    for d in data:\n",
        "      data_df.append(d)\n",
        "    data_df = pd.DataFrame(data_df)\n",
        "    data_df.dropna(inplace = True)\n",
        "    data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    return data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "d4c0pl5BAl3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "train_df = get_dataframe(train_paths[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DgMgMnisA0cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "val_df = get_dataframe(val_paths[:100])\n",
        "test_df = get_dataframe(test_paths[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>original</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/vietnews-master/data/test_tokenized/00444...</td>\n",
              "      <td>Chiến_dịch do Công_an tỉnh Sơn_La phối_hợp các...</td>\n",
              "      <td>Hai tay trùm đang trốn truy_nã điều_hành đường...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/vietnews-master/data/test_tokenized/00627...</td>\n",
              "      <td>Trước tình_trạng ngập_lụt , các quốc_gia tiên_...</td>\n",
              "      <td>Những quốc_gia tiên_tiến như Nhật_Bản , Hà_Lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/vietnews-master/data/test_tokenized/01640...</td>\n",
              "      <td>Sau 4 ngày nghị_án , sáng nay 19-6 Toà_án nhân...</td>\n",
              "      <td>Hội_đồng xét_xử xét thấy sự_cố chạy thận là hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/vietnews-master/data/test_tokenized/00352...</td>\n",
              "      <td>Dự_án cải_tạo Bến xe_buýt Tân_Quy với kinh_phí...</td>\n",
              "      <td>Trung_tâm Quản_lý giao_thông công_cộng TP. HCM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/vietnews-master/data/test_tokenized/00682...</td>\n",
              "      <td>Tờ al - Watan hôm 14/3 cho_hay , các máy_bay c...</td>\n",
              "      <td>Các đơn_vị không_quân Syria và Nga đã tiến_hàn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                file  \\\n",
              "0  data/vietnews-master/data/test_tokenized/00444...   \n",
              "1  data/vietnews-master/data/test_tokenized/00627...   \n",
              "2  data/vietnews-master/data/test_tokenized/01640...   \n",
              "3  data/vietnews-master/data/test_tokenized/00352...   \n",
              "4  data/vietnews-master/data/test_tokenized/00682...   \n",
              "\n",
              "                                            original  \\\n",
              "0  Chiến_dịch do Công_an tỉnh Sơn_La phối_hợp các...   \n",
              "1  Trước tình_trạng ngập_lụt , các quốc_gia tiên_...   \n",
              "2  Sau 4 ngày nghị_án , sáng nay 19-6 Toà_án nhân...   \n",
              "3  Dự_án cải_tạo Bến xe_buýt Tân_Quy với kinh_phí...   \n",
              "4  Tờ al - Watan hôm 14/3 cho_hay , các máy_bay c...   \n",
              "\n",
              "                                             summary  \n",
              "0  Hai tay trùm đang trốn truy_nã điều_hành đường...  \n",
              "1  Những quốc_gia tiên_tiến như Nhật_Bản , Hà_Lan...  \n",
              "2  Hội_đồng xét_xử xét thấy sự_cố chạy thận là hi...  \n",
              "3  Trung_tâm Quản_lý giao_thông công_cộng TP. HCM...  \n",
              "4  Các đơn_vị không_quân Syria và Nga đã tiến_hàn...  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_df.to_parquet(\"data/vietnews/test.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_df = pd.read_parquet(\"data/vietnews/train.parquet\")\n",
        "# val_df = pd.read_parquet(\"data/vietnews/val.parquet\")\n",
        "# test_df = pd.read_parquet(\"data/vietnews/test.parquet\")\n",
        "# test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# train_df['word_count'] = train_df['summary'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# # Plot the histogram\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(train_df['word_count'], bins=100, edgecolor='black')\n",
        "# plt.title('Histogram of Number of Words in Sentences')\n",
        "# plt.xlabel('Number of Words')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1F58j028eTV"
      },
      "source": [
        "## **Warm-starting RoBERTaShared for BBC XSum**\n",
        "\n",
        "***Note***: This notebook only uses a few training, validation, and test data samples for demonstration purposes. To fine-tune an encoder-decoder model on the full training data, the user should change the training and data preprocessing parameters accordingly as highlighted by the comments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FO5ESocXvlK"
      },
      "source": [
        "### **Data Preprocessing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "sgTiC0rhMb7C"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Vietnam_T5_small_200 does not appear to have a file named config.json. Checkout 'https://huggingface.co/Vietnam_T5_small_200/tree/None' for available files.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pretrained_model = \"google/mt5-small\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalconsai/text_summarization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVietnam_T5_small_200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:837\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
            "File \u001b[0;32m~/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:934\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    932\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 934\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    936\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
            "File \u001b[0;32m~/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
            "File \u001b[0;32m~/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/utils/hub.py:370\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m         )\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mOSError\u001b[0m: Vietnam_T5_small_200 does not appear to have a file named config.json. Checkout 'https://huggingface.co/Vietnam_T5_small_200/tree/None' for available files."
          ]
        }
      ],
      "source": [
        "# pretrained_model = \"google/mt5-small\"\n",
        "pretrained_model = \"Falconsai/text_summarization\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Vietnam_T5_small_200\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U08MrUK9LcUM"
      },
      "outputs": [],
      "source": [
        "train_data =  Dataset.from_pandas(train_df)\n",
        "val_data =  Dataset.from_pandas(val_df)\n",
        "test_data =  Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "70bb7c0669ca4a3699ad36dfdcc10910",
            "1ba158d2f09a4c54bc647ebe77d27b60",
            "5ac8ad6eea254d369996622911a6a79e",
            "0aeb387a0f53469c8ff42e57647831d8",
            "efd6b87b93244d5ca6817ab35c385510",
            "e22006901f10483882e1a1f20f645b24",
            "01644f724526402c9f139e19b2035073",
            "7097c890712d418e9dd9c3e33873a2d5",
            "b3ddde0882d841daa3bfde43ed6e6fc9",
            "09375b2aac814adcbb51589402eb8b68",
            "cab0cd369bfa4c80b963ebd27d3e4974",
            "db0264404d934633824b5b381b31a5ac",
            "63276058881e45539d8dcd4ff2e05edd",
            "03b57581ace346d2bde4378a7c6309ca",
            "5507016090f64bb99dcbf88e14d71eda",
            "0d6a0b11ec2a4c6d975857678415f505"
          ]
        },
        "id": "yoN2q0hZUbXN",
        "outputId": "20c6562f-7358-4dc7-dada-787bede963bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 105418/105418 [01:29<00:00, 1176.63 examples/s]\n",
            "Map: 100%|██████████| 5000/5000 [00:04<00:00, 1179.01 examples/s]\n"
          ]
        }
      ],
      "source": [
        "batch_size=8  # change to 16 for full training\n",
        "\n",
        "max_input_length = 512 #1024\n",
        "max_target_length = 128 #128\n",
        "\n",
        "\n",
        "def process_data_to_model_inputs(batch):\n",
        "    model_inputs = tokenizer(\n",
        "        batch[\"original\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        batch[\"summary\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# only use 32 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
        "# train_data = train_data.select(range(32))\n",
        "\n",
        "train_data_batch = train_data.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"file\",\"original\", \"summary\"],\n",
        ")\n",
        "train_data_batch.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "\n",
        "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
        "# val_data = val_data.select(range(16))\n",
        "\n",
        "val_data_batch = val_data.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"file\", \"original\", \"summary\"],\n",
        ")\n",
        "val_data_batch.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "file        data/vietnews-master/data/train_tokenized/0848...\n",
              "original    Dự_án nào đang vướng thủ_tục pháp_lý , dự_án n...\n",
              "summary     Những ngày qua , thông_tin hơn 100 dự_án bất_đ...\n",
              "Name: 0, dtype: object"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Những ngày qua , thông_tin hơn 100 dự_án bất_động_sản ở TP. HCM bị \" đóng_băng \" , rồi lại đến tin 124 dự_án được tháo_gỡ , có_khi là 160 dự_án vướng_mắc … khiến những người đã và đang có ý_định mua căn_hộ càng lo_lắng . \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': [445, 107, 2, 1725, 3, 1725, 85, 63, 3, 4960, 3, 6, 3, 189, 10079, 1725, 834, 17, 77, 3, 107, 2, 29, 910, 3, 26, 2, 834, 12916, 3, 115, 2, 17, 834, 2, 1725, 834, 7, 2, 29, 3, 2, 3, 7150, 5, 454, 5518, 3, 115, 2, 96, 3, 2, 15742, 122, 834, 115, 98, 1725, 96, 3, 6, 3, 52, 2, 23, 3, 40, 2, 23, 3, 2, 29, 3, 17, 77, 3, 22504, 3, 26, 2, 834, 12916, 3, 2, 75, 3, 189, 2975, 32, 834, 122, 2, 3, 6, 3, 75, 4922, 834, 29392, 5381, 11321, 3, 26, 2, 834, 12916, 3, 208, 2, 1725, 834, 51, 2, 75, 3, 233, 3, 29392, 2, 29, 3, 29, 107, 2, 1725, 3, 1725, 2, 23, 3, 2, 3, 208, 85, 3, 2, 1468, 3, 75, 4922, 3, 2, 834, 2, 29, 107, 4035, 9, 375, 29, 834, 107, 2, 3, 75, 85, 1725, 6899, 834, 40, 2, 1725, 3, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = tokenizer(str(train_df[\"summary\"].iloc[0]))\n",
        "print(str(train_df[\"summary\"].iloc[0]))\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁N', 'h', '<unk>', 'ng', '▁', 'ng', 'à', 'y', '▁', 'qua']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(train_data_batch[0][\"labels\"])[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_data_batch[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEjb026cNC38"
      },
      "source": [
        "### **Warm-starting the Encoder-Decoder Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generation_config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, GenerationConfig\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model).to(device)\n",
        "my_config = model.generation_config\n",
        "print(\"generation_config\", model.generation_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u98CLZiTkgzv"
      },
      "source": [
        "### **Fine-Tuning Warm-Started Encoder-Decoder Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gYzA-w96wCt"
      },
      "source": [
        "The `Seq2SeqTrainer` that can be found under [examples/seq2seq/seq2seq_trainer.py](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/seq2seq_trainer.py) will be used to fine-tune a warm-started encoder-decoder model.\n",
        "\n",
        "Let's download the `Seq2SeqTrainer` code and import the module along with `TrainingArguments`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nmQRT3XuHHz"
      },
      "source": [
        "We need to add some additional parameters to make `TrainingArguments` compatible with the `Seq2SeqTrainer`. Let's just copy the `dataclass` arguments as defined in [this file](https://github.com/patrickvonplaten/transformers/blob/make_seq2seq_trainer_self_contained/examples/seq2seq/finetune_trainer.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPUAgo7pxH24"
      },
      "source": [
        "Also, we need to define a function to correctly compute the ROUGE score during validation. ROUGE is a much better metric to track during training than only language modeling loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Decode generated summaries into text\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    # Decode reference summaries into text\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge_score.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "    # Extract the median scores\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/data/data_collator.py:646: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 309,    2,  834,  ..., 1725,  834,    1],\n",
              "        [ 445,  122,   85,  ..., 2975,  102,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  445,   107,     2,  1725,     3,  1725,    85,    63,     3,  4960,\n",
              "             3,     6,     3,   189, 10079,  1725,   834,    17,    77,     3,\n",
              "           107,     2,    29,   910,     3,    26,     2,   834, 12916,     3,\n",
              "           115,     2,    17,   834,     2,  1725,   834,     7,     2,    29,\n",
              "             3,     2,     3,  7150,     5,   454,  5518,     3,   115,     2,\n",
              "            96,     3,     2, 15742,   122,   834,   115,    98,  1725,    96,\n",
              "             3,     6,     3,    52,     2,    23,     3,    40,     2,    23,\n",
              "             3,     2,    29,     3,    17,    77,     3, 22504,     3,    26,\n",
              "             2,   834, 12916,     3,     2,    75,     3,   189,  2975,    32,\n",
              "           834,   122,     2,     3,     6,     3,    75,  4922,   834, 29392,\n",
              "          5381, 11321,     3,    26,     2,   834, 12916,     3,   208,     2,\n",
              "          1725,   834,    51,     2,    75,     3,   233,     3, 29392,     2,\n",
              "            29,     3,    29,   107,     2,  1725,     3,     1],\n",
              "        [ 7080,  1725,   204,     3,   189, 12916,   122,     3,   122,     2,\n",
              "            29,     3,     2,  1439,    63,     3,    29,   107,     2,    17,\n",
              "             3,     6,     3,    75,     2,   834,  4960,    29,     3,    75,\n",
              "         10079,  1725,   834,   152,     3,   208,    85,     3,    75,  2975,\n",
              "            75,     3,  1725,    85,    29,   107,     3,   524,     2,    75,\n",
              "           834,    29,    98,  1725,     3,     2,     3,  7150,     5,     3,\n",
              "             2,    85,   834,   567,     2,  1725,     3,     2,     3,   122,\n",
              "             2,     3, 28569,     2,  1725,     3, 29166,     3,    17,     2,\n",
              "           834,    52,     2,    23,     3,    17,     2,    29,   834,    26,\n",
              "             2,  1725,     3,     2,    35,     3,     5,     1,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[    0,   445,   107,     2,  1725,     3,  1725,    85,    63,     3,\n",
              "          4960,     3,     6,     3,   189, 10079,  1725,   834,    17,    77,\n",
              "             3,   107,     2,    29,   910,     3,    26,     2,   834, 12916,\n",
              "             3,   115,     2,    17,   834,     2,  1725,   834,     7,     2,\n",
              "            29,     3,     2,     3,  7150,     5,   454,  5518,     3,   115,\n",
              "             2,    96,     3,     2, 15742,   122,   834,   115,    98,  1725,\n",
              "            96,     3,     6,     3,    52,     2,    23,     3,    40,     2,\n",
              "            23,     3,     2,    29,     3,    17,    77,     3, 22504,     3,\n",
              "            26,     2,   834, 12916,     3,     2,    75,     3,   189,  2975,\n",
              "            32,   834,   122,     2,     3,     6,     3,    75,  4922,   834,\n",
              "         29392,  5381, 11321,     3,    26,     2,   834, 12916,     3,   208,\n",
              "             2,  1725,   834,    51,     2,    75,     3,   233,     3, 29392,\n",
              "             2,    29,     3,    29,   107,     2,  1725,     3],\n",
              "        [    0,  7080,  1725,   204,     3,   189, 12916,   122,     3,   122,\n",
              "             2,    29,     3,     2,  1439,    63,     3,    29,   107,     2,\n",
              "            17,     3,     6,     3,    75,     2,   834,  4960,    29,     3,\n",
              "            75, 10079,  1725,   834,   152,     3,   208,    85,     3,    75,\n",
              "          2975,    75,     3,  1725,    85,    29,   107,     3,   524,     2,\n",
              "            75,   834,    29,    98,  1725,     3,     2,     3,  7150,     5,\n",
              "             3,     2,    85,   834,   567,     2,  1725,     3,     2,     3,\n",
              "           122,     2,     3, 28569,     2,  1725,     3, 29166,     3,    17,\n",
              "             2,   834,    52,     2,    23,     3,    17,     2,    29,   834,\n",
              "            26,     2,  1725,     3,     2,    35,     3,     5,     1,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]])}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = [train_data_batch[i] for i in range(2)]\n",
        "data_collator(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ik4hZb2yV-b"
      },
      "source": [
        "Cool! Finally, we start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "LAaTxUpdzshF",
        "outputId": "7e103b00-0ac7-41c0-84a4-884932474b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logging_steps 2000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='98762' max='131780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 98762/131780 7:52:20 < 2:37:55, 3.48 it/s, Epoch 7.49/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.539300</td>\n",
              "      <td>1.422631</td>\n",
              "      <td>15.957100</td>\n",
              "      <td>5.680300</td>\n",
              "      <td>13.707100</td>\n",
              "      <td>13.812700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.425500</td>\n",
              "      <td>1.342705</td>\n",
              "      <td>16.303900</td>\n",
              "      <td>6.075700</td>\n",
              "      <td>14.003100</td>\n",
              "      <td>14.111400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.374700</td>\n",
              "      <td>1.292822</td>\n",
              "      <td>16.285200</td>\n",
              "      <td>6.139000</td>\n",
              "      <td>14.011900</td>\n",
              "      <td>14.120900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.334700</td>\n",
              "      <td>1.262241</td>\n",
              "      <td>16.448100</td>\n",
              "      <td>6.271400</td>\n",
              "      <td>14.170600</td>\n",
              "      <td>14.280600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.318600</td>\n",
              "      <td>1.239643</td>\n",
              "      <td>16.521300</td>\n",
              "      <td>6.430700</td>\n",
              "      <td>14.289000</td>\n",
              "      <td>14.385300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.297300</td>\n",
              "      <td>1.224418</td>\n",
              "      <td>16.424400</td>\n",
              "      <td>6.384000</td>\n",
              "      <td>14.216700</td>\n",
              "      <td>14.318800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.281700</td>\n",
              "      <td>1.213849</td>\n",
              "      <td>16.572700</td>\n",
              "      <td>6.468500</td>\n",
              "      <td>14.355800</td>\n",
              "      <td>14.470300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 35\u001b[0m\n\u001b[1;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m OUTPUT_DIR,\n\u001b[1;32m      7\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     26\u001b[0m     model,\n\u001b[1;32m     27\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/trainer.py:1876\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1877\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
            "File \u001b[0;32m~/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# set training arguments - these params are not really tuned, feel free to change\n",
        "logging_steps = min(2000,len(train_data_batch) // batch_size)\n",
        "print(\"logging_steps\", logging_steps)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir= OUTPUT_DIR,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    predict_with_generate=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    # do_train=True,\n",
        "    # do_eval=True,\n",
        "    logging_steps=logging_steps,  # set to 2000 for full training\n",
        "    save_steps=500,  # set to 500 for full training\n",
        "    eval_steps=750,  # set to 7500 for full training\n",
        "    warmup_steps=3000,  # set to 3000 for full training\n",
        "    num_train_epochs=10, #uncomment for full training\n",
        "    overwrite_output_dir=True,\n",
        "    optim='adafactor',\n",
        "    save_total_limit=3,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data_batch,\n",
        "    eval_dataset=val_data_batch,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.safetensors: 100%|██████████| 242M/242M [00:38<00:00, 6.26MB/s] \n"
          ]
        }
      ],
      "source": [
        "trainer.save_model(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7854KKs6EY4x"
      },
      "outputs": [],
      "source": [
        "# !gsutil -m cp -r '/content/training/*' 'gs://kaggle-vbdi-test/training_Data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwQIEhKOrJpl"
      },
      "source": [
        "### **Evaluation**\n",
        "\n",
        "Awesome, we finished training our dummy model. Let's now evaluated the model on the test data. We make use of the dataset's handy `.map()` function to generate a summary of each sample of the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "0ef7f4e43319429d9277d55c83cb084d",
            "ab543b6ef5b34ed4acb85fd25304ed75",
            "825d4b325e9b45859e808cd6b14fdd42",
            "dd0bdf658c6d4c0cb0a091129ecbebb0",
            "f7c296a0b8a84ee3be45c92edb6aad21",
            "e21d97b40ffa43668b9962c5d771debb",
            "a4898ec38618401396928eee2c4f9926",
            "3598a3c015a64c20be134cf4d2e7fbbe"
          ]
        },
        "id": "oOoSrwWarJAC",
        "outputId": "67b60b74-c47c-4718-82a7-072f85a32c4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Parameter 'function'=<function generate_summary at 0x7f954f5cecb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function generate_summary at 0x7f954f5cecb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generation_config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]/home/mrzaizai2k/code_Bao/ViT5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 100/100 [00:01<00:00, 50.35 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model, use_fast=False)\n",
        "\n",
        "# model = EncoderDecoderModel.from_pretrained(OUTPUT_DIR + \"/checkpoint-4000\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(OUTPUT_DIR, torch_dtype=torch.bfloat16,).to(device)\n",
        "print(\"generation_config\", model.generation_config)\n",
        "\n",
        "\n",
        "# test_data = datasets.load_dataset(\"xsum\", split=\"test\")\n",
        "\n",
        "batch_size = 16  # change to 64 for full evaluation\n",
        "\n",
        "# map data correctly\n",
        "def generate_summary(batch):\n",
        "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
        "    inputs = tokenizer(batch[\"original\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask,\n",
        "                             max_length=128,\n",
        "                        # num_beams=4,\n",
        "                        # no_repeat_ngram_size=3,\n",
        "                        # early_stopping=True,\n",
        "                        )\n",
        "\n",
        "    # all special tokens including will be removed\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    batch[\"pred\"] = output_str\n",
        "\n",
        "    return batch\n",
        "\n",
        "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"original\"])\n",
        "\n",
        "pred_str = results[\"pred\"]\n",
        "label_str = results[\"summary\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHl8NMjEiTb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge1': 11.615472530587926,\n",
              " 'rouge2': 2.5109031615037156,\n",
              " 'rougeL': 9.59317086900173,\n",
              " 'rougeLsum': 9.598338898153589}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rouge_output = rouge_score.compute(predictions=pred_str, references=label_str, use_stemmer=True)\n",
        "rouge_output = {key: value * 100 for key, value in rouge_output.items()}\n",
        "rouge_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8R5CclwUGuC",
        "outputId": "9b53d98d-8d04-49b5-c55a-d47530564398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prediction:  Các nhà_t_chc cho_hay\n",
            "Truth:  Hàng trăm_nghìn người Hàn_Quốc hôm_nay xuống_đường ở Seoul trong cuối tuần thứ tư liên_tiếp để phản_đối Tổng_thống Park_Geun-hye sau bê_bối liên_quan tới bạn thân . \n",
            "Content:  Các nhà_tổ_chức cho_hay có 500.000 người đã tham_gia biểu_tình tối nay . Cuộc biểu_tình tối nay có quy_mô nhỏ hơn những lần trước khi các hoạt_động tương_tự cũng diễn ra ở khắp nhiều thành_phố khác . Cảnh_sát cho_hay ít_nhất 155.000 người đã tập_trung tại một quảng_trường trung_tâm Seoul vào đầu buổi tối để thắp nến , trong khi các nhà_tổ_chức nói rằng con_số này lên tới 500.000 người . Tổng_thống Park từ_chối các lời kêu_gọi từ_chức , bất_chấp cuộc khủng_hoảng_chính_trị đang diễn ra , trong đó bà bị_cáo buộc để Choi_Soon - sil , một người bạn thân , can_thiệp vào công_việc nhà_nước và trục_lợi cho cá_nhân . Bà thừa_nhận sai_sót trên và cam_kết hợp_tác với các nhà điều_tra vụ bê_bối . Các công_tố_viên dự_kiến sẽ trình cáo_trạng chống lại bà Choi và hai cựu cố_vấn tổng_thống vào ngày_mai . Người biểu_tình tại trung_tâm Seoul tối nay . Tuy_nhiên , không tất_cả người Hàn_Quốc đều kêu_gọi tổng_thống từ_chức . Cách điểm biểu_tình chính không xa , một nhóm người bảo_thủ tập_trung bên ngoài ga Seoul để ủng_hộ bà Park . \" 16 triệu người đã bầu ra tổng_thống này . Điều đó có nghĩa_là không_thể yêu_cầu bà rút_lui một_cách đơn_giản được \" , Geum Sang - chul , một người nghỉ hưu 78 tuổi , thành_viên của Hội Cựu_chiến_binh Hàn_Quốc , nói . Ông Geum tham_gia một nhóm chống người biểu_tình mà cảnh_sát ước_tính có khoảng 11.000 người . Nhiều người trong số 5 % người_dân Hàn_Quốc còn ủng_hộ bà Park là những người trung_thành với cha bà , cố tổng_thống Park_Chung - hee , người từng lãnh_đạo nước này suốt 18 năm cho đến khi bị ám_sát năm 1979 . \" Nếu họ thực_sự quan_tâm đến đất_nước , họ nên cân_nhắc đến hình_ảnh của nó \" , Lee Sang - soon , một người nghỉ hưu 66 tuổi , nói . \" Tôi rất buồn khi đất_nước được nhắc đến ở nước_ngoài bằng những cuộc biểu_tình trên \" . Xem thêm : Hàn_Quốc sẽ thẩm_vấn Tổng_thống Park_Geun-hye Anh Ngọc   Các nhà_tổ_chức cho_hay có 500.000 người đã tham_gia biểu_tình tối nay . \n",
            "\n",
            "Prediction:  p tinh_khôi mà m\n",
            "Truth:  Chọn cho mình những chiếc áo_dài trắng thướt_tha , “ ngọc nữ điện_ảnh ” Kim_Tuyến khiến nhiều người ngẩn_ngơ trước vẻ_đẹp không tỳ vết . \n",
            "Content:  Trong những ngày đầu năm , Kim_Tuyến “ gây thương_nhớ ” bằng vẻ đẹp tinh_khôi mà đậm chất truyền_thống . Nữ diễn_viên xuất_hiện dịu_dàng trong những chiếc áo_dài duyên_dáng cũng như mang đến chút hương_vị dân dã hay một_chút không_khí của xuân Đinh_Dậu . Người đẹp luôn tinh_tế và khéo_léo trong việc xây_dựng hình_ảnh của chính mình . Đó chính là những hình_ảnh biến_hoá được nhiều lời khen_ngợi từ công_chúng . Và lần này cũng không ngoại_lệ , Kim_Tuyến nền_nã , duyên_dáng và đậm chất riêng trong những bức hình chào năm mới . Kim_Tuyến thổi hồn quê vào những chiếc áo_dài . Nét hồn_hậu , duyên_dáng của người phụ_nữ Việt được Kim_Tuyến gửi_gắm trong những khoảnh_khắc cuốn_hút nhất của ngày xuân . Trên nền chiếc áo_dài trắng tinh_khôi là những hoạ_tiết mang đậm hồn quê cũng như phảng_phất không_khí của xuân Đinh_Dậu . Hình_ảnh con gà , bụi chuối , phụ_nữ , … lần_lượt trở_thành những điểm nhấn trên tà áo_dài truyền_thống . Bên cạnh đó , những biểu_cảm vô_cùng tinh_tế của Kim_Tuyến cũng giúp những hình_ảnh này trở_nên cuốn_hút hơn_bao_giờ_hết . Hiện_tại , Kim_Tuyến đang được mọi người chú_ý với vai diễn đầy thú_vị trong phim Lục_Vân_Tiên của đạo_diễn Hoàng_Phúc . Với tạo_hình ấn_tượng cũng như chọn cho mình một hình_ảnh mới_mẻ , Kim_Tuyến đang nhận được nhiều phản_hồi tích_cực từ khán_giả . Trong bộ phim Lục_Vân_Tiên , Kim_Tuyến hoá_thân với hình_ảnh đáng nhớ là cô_gái quê , một nhân_vật khác hoàn_toàn so với truyện gốc để mang đến những điều bất_ngờ cho khán_giả . Được biết , đây là vai diễn tâm_huyết của nữ diễn_viên từ lúc hoá_thân đến tạo_hình ra_mắt bộ phim . Có_thể nói , Kim_Tuyến là một trong những mỹ nữ tài_sắc của điện_ảnh nói_riêng và làng giải_trí nói_chung . Cô sở_hữu nét đẹp không tỳ vết , biến_hoá đa_dạng trong hình_ảnh để mang đến những nét riêng đáng nhớ . Song_Ngư  Kim_Tuyến hút_hồn người đối_diện bằng vẻ ngoài tinh_khôi . \n",
            "\n",
            "Prediction:  Lc_lng c_nhim Nga\n",
            "Truth:  Quân_đội Nga đã triển_khai bổ_sung các đơn_vị đặc_nhiệm đến khu_vực chiến_trường ở Syria sau sự_cố Su -25 bị bắn rơi ở tỉnh Idlib . \n",
            "Content:  Lực_lượng đặc_nhiệm Nga tại Syria . Lực_lượng đặc_nhiệm Nga đã được triển_khai hoạt_động ở các tỉnh Hama , Aleppo , Deir ez - Zor và khu_vực giao_tranh giữa quân_đội Syria và phiến quân Hayat_Tahrir al - Sham , nhóm nhận bắn rơi cường_kích Su -25 , tại tỉnh Idlib , South_Front ngày 7/2 đưa tin . Ngoài_ra , nhiều cố_vấn quân_sự Nga cũng xuất_hiện tại căn_cứ không_quân Abu al - Duhur của quân_đội Syria thuộc tỉnh Idlib . Cường_kích Su -25 Nga ngày 3/2 bị phiến quân Hồi_giáo cực_đoan bắn rơi bằng tên_lửa phòng_không vác vai ( MANPAD ) , khiến thiếu_tá phi_công Nga Roman_Filipov thiệt_mạng . Vụ_việc đã buộc Nga phải ngừng triển_khai máy_bay chiến_đấu Nga hoạt_động ở tỉnh Idlib trong thời_gian chờ điều_tra nguồn_gốc tên_lửa . Tuy_nhiên , Moscow chuyển sang sử_dụng tên_lửa hành_trình trên tàu_chiến để tiếp_tục tấn_công nhằm vào mục_tiêu phiến quân , báo_thù cho phi_công Su -25 . Nguyễn_Hoàng   Lực_lượng đặc_nhiệm Nga tại Syria . \n",
            "\n",
            "Prediction:  iu_hành phiên tho_\n",
            "Truth:  Quốc_hội rút ngắn thời_lượng thảo_luận Luật giáo_dục sửa_đổi từ 1 ngày xuống 1 buổi , trong khi bảng điện_tử hiện 63 đại_biểu đăng_ký phát_biểu . \n",
            "Content:  Điều_hành phiên thảo_luận sáng nay 15-11 , Phó chủ_tịch Quốc_hội Tòng_Thị_Phóng cho_biết thời_gian tới , Uỷ_ban Thường_vụ_Quốc_hội sẽ tổ_chức các hội_nghị đại_biểu chuyên_trách , thảo_luận chuyên_đề để tiếp_nhận các ý_kiến đóng_góp cho dự_án luật này trước khi Quốc_hội xem_xét thông_qua giữa năm 2019 . Sản_phẩm giáo_dục chưa đáp_ứng yêu_cầu xã_hội \" Không hiếm trường_hợp sinh_viên ra trường không viết nổi một văn_bản , nhiều doanh_nghiệp phải đào_tạo bổ_sung nếu tuyển_dụng sinh_viên mới tốt_nghiệp . Nhà_trường chưa dành nhiều thời_gian đào_tạo kỹ_năng mềm , người học thiếu tính chủ_động , nên sản_phẩm giáo_dục chưa đáp_ứng yêu_cầu của nhà_tuyển_dụng và xã_hội \" , đại_biểu Phạm_Trọng_Nhân ( Bình_Dương ) nhận_định . Ông Nhân cho_rằng \" việc cắp sách đến trường chỉ mới dừng lại ở nghĩa_vụ mà chưa phải là một niềm_vui , niềm khao_khát được hướng_dẫn để tiếp_cận kho_tàng_tri_thức của nhân_loại \" . \" Một trong những mục_tiêu của giáo_dục là để hội_nhập quốc_tế , nhưng luật và nghị_định không có bất_kỳ điều_khoản nào quy_định ngoại_ngữ là một công_cụ bắt_buộc , hình_thành nền_tảng cơ_bản nhất cho hội_nhập . Trẻ_em Việt_Nam được học tiếng Anh từ rất sớm , nhưng rất nhiều trường_hợp không_thể sử_dụng tiếng Anh sau khi tốt_nghiệp THPT \" , đại_biểu Bình_Dương trăn_trở . Ông Nhân sốt_ruột khi \" nhìn các bạn trẻ sinh những năm 2000 , khi đất_nước đã hội_nhập , vẫn rất vất_vả sử_dụng tiếng Anh \" , và lo_lắng mục_tiêu hội_nhập quốc_tế của giáo_dục còn nhiều chông_gai nếu không chế_định tiếng Anh là công_cụ bắt_buộc trong dạy_học như Singapore hay Philippines đã làm . ' Luật thiếu triết_lý , chỉ giải_quyết sự_vụ ' Vẫn theo đại_biểu Phạm_Trọng_Nhân , \" 4 trụ_cột trong mục_tiêu , triết_lý giáo_dục của dự_thảo luật có nhiều điểm hay nhưng toàn_bộ các điều_khoản sau đó không xoay quanh 4 trụ_cột này mà hầu_như chỉ tập_trung giải_quyết các sự_vụ , sự_việc \" . Ông Nhân phân_tích : \" Để đáp_ứng bối_cảnh mới , với sự thay_đổi chóng_mặt của khoa_học , kỹ_thuật , công_nghệ thì một trong những phương_pháp giáo_dục phải là khai phóng , hướng đến từng người học , giúp các em chủ_động trong nhận_thức , độc_lập trong tư_duy sáng_tạo , để có_thể tự kết_nối , tự kiểm_soát và thích_ứng với những thay_đổi của thế_giới hiện_nay . Để đạt được yêu_cầu này , sứ_mệnh của giáo_dục là phá_bỏ tư_duy , thói_quen đã ăn sâu vào tiềm_thức trong dạy và học bấy_lâu . Theo đó , những gì là tinh_hoa của nhân_loại , dân_tộc và thời_đại cần phải được chắt_lọc để hướng_dẫn thế_hệ trẻ . Muốn đất_nước sánh_vai với các cường_quốc thì ít_nhất phải tạo được nền_tảng vững_chắc từ đạo_luật này \" . Trong 7 phút phát_biểu , đại_biểu tỉnh Bình_Dương say_sưa trình_bày : \" Chất_liệu chính của một triết_lý giáo_dục chỉ có_thể là sự hướng_thiện con_người đi cùng với những ưu_tư , trăn_trở của mình về trách_nhiệm trước vận_mệnh_thịnh_suy của đất_nước . Một nền giáo_dục thiếu triết_lý như thiếu ngọn hải_đăng dẫn đường . Thiếu triết_lý giáo_dục cũng đồng_thời làm cho đất_nước thiếu đi một bộ_phận cấu_thành của triết_lý phát_triển \" . Cùng trăn_trở về triết_lý giáo_dục , đại_biểu Cao_Đình_Thưởng ( Phú_Thọ ) dẫn lại lời Bác_Hồ - học để làm_việc , làm người , làm cán_bộ , để phụng_sự đồng_bào … , muốn vậy phải cần , kiệm , liêm , chính , chí_công , vô_tư . Ông Thưởng cho_rằng đây chính là căn_cốt để tìm ra triết_lý giáo_dục cho Việt_Nam . Đại_biểu Phú_Thọ không khỏi tâm_tư khi bình_luận về hiện_trạng giáo_dục nước_nhà : \" Người_lớn nghĩ ra quá nhiều điều để nhồi_nhét vào bộ_óc non_nớt của trẻ , tạo ra gánh nặng , làm cho trẻ_con sợ học \" . \" Không_thể bắt trẻ học để trở_thành ông nọ bà kia khi các cháu không thích . Trong nhà_trường chỉ một_vài em thành nhà_văn , một_vài em thành nghệ_sĩ , một_vài em làm vận_động_viên … Hãy định_hướng cho các em học để phát_huy năng_khiếu , sở_trường của mình \" , ông Thưởng nói . * Tuổi_Trẻ Online tiếp_tục cập_nhật nội_dung phiên thảo_luận .  Đại_biểu Phạm_Trọng_Nhân cho_rằng vẫn đang thiếu một \" ngọn hải_đăng \" cho nền giáo_dục - Ảnh : Quochoi.vn . Đại_biểu Trần_Thị_Hiền phát_hiện một khiếm_khuyết của dự_thảo Luật giáo_dục sửa_đổi là đang \" bỏ quên \" người khuyết_tật - Ảnh : Quochoi.vn \n",
            "\n",
            "Prediction:  ng_c ca Gôn châu_\n",
            "Truth:  Gôn vốn được coi là môn thể_thao có tính nhân_văn đặc_biệt khi những đối_thủ thay_vì đối_đầu mà cùng nhau chiến_thắng bản và hướng về phía trước . \n",
            "Content:  Là doanh_nghiệp phát_triển gắn liền với Gôn , tập_đoàn BRG đã mang đến những giá_trị nhân_ái đầy ý_nghĩa cho người_yêu Gôn không_chỉ tại Việt_Nam mà_còn lan_toả đến cộng_đồng những người làm Gôn quốc_tế . Hội_nghị Gôn châu_Á Thái_Bình_Dương 2017 ( APGS 2017 ) khép lại với sự thành_công và khẳng_định chỗ_đứng của Gôn Việt_Nam trên bản_đồ khu_vực . Vượt ra những giá_trị chuyên_môn , nơi đây còn ghi_nhận tính nhân_văn qua sự đồng_cảm về thực_hiện trách_nhiệm xã_hội của những doanh_nhân và người đam_mê Gôn chân_chính . Trong khuôn_khổ APGS 2017 , tỷ_phú Bob_Parsons , nhà sáng_lập hãng dụng_cụ Gôn nổi_tiếng PXG đã cam_kết ủng_hộ trẻ_em nghèo Việt_Nam số tiền lên đến 200 nghìn đô_la Mỹ thông_qua các chương_trình từ_thiện do tập_đoàn BRG triển_khai và do Madame Nguyễn_Thị_Nga , Chủ_tịch tập_đoàn BRG lựa_chọn . Ngay sau thông_tin đầy ý_nghĩa tại APGS 2017 , BRG đã tiếp_tục tham_gia đồng tổ_chức giải Golf Tiền_Phong_Championship 2017 lần thứ nhất diễn ra vào ngày 16/12 tại BRG King ’ s Island Golf_Resort ( Hà_Nội ) . Với mục_đích giúp_đỡ các tài_năng trẻ tham_gia tích_cực vào sự_nghiệp xây_dựng và bảo_vệ tổ_quốc . Phát_hiện , cổ_vũ và hỗ_trợ đối_với những thanh_niên có năng_khiếu để tạo nguồn nhân_tài trên các lĩnh_vực , nhất_là đối_với những tài_năng trẻ có hoàn_cảnh khó_khăn . Một lần nữa , những người có chung niềm đam_mê Gôn có cơ_hội cùng nhau thực_hiện trách_nhiệm xã_hội cao_cả , đóng_góp những điều tốt_đẹp cho xã_hội . Chia_sẻ về ý_nghĩa của những hoạt_động từ_thiện của tập_đoàn BRG thông_qua thể_thao Gôn , Madame Nguyễn_Thị_Nga , Chủ_tịch tập_đoàn BRG chia_sẻ : “ Phong_trào chơi Gôn tại Việt_Nam đang phát_triển mạnh_mẽ và tạo được chỗ_đứng trên bản_đồ gôn quốc_tế . Số_lượng Gôn thủ trong nước ngày_một tăng lên , cùng với đó rất nhiều những gôn thủ nước_ngoài lựa_chọn Việt_Nam là điểm đến . Thông_qua những sự_kiện Gôn , ngoài niềm đam_mê thể_thao được chia_sẻ , lòng nhân_ái cũng được thắp lên mạnh_mẽ , và để lan_toả những giá_trị nhân_văn của môn thể_thao hấp_dẫn này , tập_đoàn BRG chúng_tôi luôn mong_muốn được cùng với Gôn đóng_góp cho xã_hội những điều tốt_đẹp nhất . ” Nhắc đến những hoạt_động từ_thiện của tập_đoàn BRG thông_qua thể_thao Gôn phải kể đến “ Swing for the Kids , ” đây là chương_trình từ_thiện mà tập_đoàn BRG đã phối_hợp với báo Đầu_tư trong nhiều năm qua nhằm hỗ_trợ tới các em học_sinh , sinh_viên có hoàn_cảnh khó_khăn , đặc_biệt là con_em các đồng_bào dân_tộc_thiểu_số , vùng_sâu , vùng_xa , miền núi và hải_đảo . Trên hành_trình đưa các suất học_bổng đến tận_tay các em học_sinh cả nước , Swing for the Kids đã và đang đồng_hành cùng Madame Nguyễn_Thị_Nga và các sân gôn của tập_đoàn BRG . Niềm đam_mê Gôn cùng tình yêu_thương đến những trẻ_em có hoàn_cảnh khó_khăn đã khiến Madame_Nga chia_sẻ : “ Chúng_tôi sẽ đồng_hành cùng Swing for the Kids mãi_mãi ” . Nguyễn_Hà  \n",
            "\n",
            "Prediction:  Xác_nhn vi Tui_Tr\n",
            "Truth:  Một vụ_việc hi_hữu vừa xảy ra tại Trại tạm giam T16 Bộ Công_an . Hai tử_tù Lê_Văn_Thọ và Nguyễn_Văn_Tình đã trốn ra khỏi phòng biệt giam . \n",
            "Content:  Ngày 13-9 , nguồn_tin của Tuổi_Trẻ Online từ Bộ Công_an cho_biết hai tử_tù đang bị giam tại trại tạm giam T16 để chờ thi_hành án đã bỏ trốn khỏi phòng biệt giam . Xác_nhận với Tuổi_Trẻ Online , một giám_thị có trách_nhiệm tại trại tạm giam T16 cho_biết 2 tử_tù bỏ trốn vào đêm 10-9 . Ngay sau khi vụ_việc xảy ra , trại tạm giam đã báo_cáo lên Bộ công_an để xin phương_án truy_tìm các tử_tù này . Theo nguồn_tin của Tuổi_Trẻ Online , danh_tính của 2 tử_tù được xác_định là Lê_Văn_Thọ ( biệt_danh Thọ sứt ) 37 tuổi , trú tại xóm 6 , xã Thanh_Sơn , huyện Thanh_Hà , Hải_Dương và Nguyễn_Văn_Tình , trú tại xã Đông_Xuân , Quốc_Oai , Hà_Nội . Thọ có 4 tiền_án về các tội giết người , bắt_cóc nhằm chiếm_đoạt tài_sản , sử_dụng trái_phép vũ_khí quân_dụng , đưa hối_lộ , trộm_cắp tài_sản , lạm_dụng tín_nhiệm chiếm_đoạt tài_sản . Tình có 1 tiền_án về tội tiêu_thụ tài_sản do người khác phạm_tội mà có . Vào tháng 5-2017 , Toà_án nhân_dân ( TAND ) tỉnh Hà_Nam đã tuyên_phạt Lê_Văn_Thọ mức án tử_hình cho các tội_danh mua_bán trái_phép chất ma_tuý , giết người và lừa_đảo chiếm_đoạt tài_sản . Theo hồ_sơ vụ án , mặc_dù là bị án đang chấp_hành án phạt tù có thời_hạn tại trại tạm giam Nam_Hà của Bộ Công_an nhưng Thọ vẫn gọi điện thuê và chỉ_đạo các đối_tượng ngoài xã_hội thực_hiện hành_vi giết người , lừa_đảo chiếm_đoạt tài_sản , mua_bán trái_phép chất ma_tuý . Nguyễn_Văn_Tình bị TAND TP Hà_Nội xử mức án tử_hình trong một vụ buôn_bán ma_tuý vào tháng 4-2017 . Tuổi_Trẻ Online đã đặt câu_hỏi về việc theo quy_định tử_tù bị giam trong phòng biệt giam sẽ bị cùm chân thì sao vẫn có_thể bỏ trốn nhưng đại_diện trại tạm giam T16 chưa trả_lời và chỉ nói ngắn_gọn : \" cơ_quan công_an đang có phương_án để truy bắt \" . Được biết Công_an TP Hà_Nội đã có thông_báo về việc truy_tìm hai đối_tượng bỏ trốn .  Chân_dung 2 tử_tù bỏ trốn - Ảnh : Công_an Hà_Nội \n",
            "\n",
            "Prediction:  ti qun Nishi ca Nagoya\n",
            "Truth:  Câu_chuyện được báo The_Mainichi thuật lại như một hiện_tượng thiếu người thừa_kế ở các doanh_nghiệp nhỏ và vừa của Nhật . \n",
            "Content:  \" Miễn_là còn khách_hàng , tôi có trách_nhiệm tiếp_tục duy_trì hoạt_động của công_ty . Giờ_đây tôi cảm_thấy nhẹ_nhõm vì đã biết rõ người kế_nhiệm mình sẽ là ai \" - ông Yasutaka_Nagao , 72 tuổi , ông chủ của công_ty nhỏ có tên Nagao_Shiko , chia_sẻ . Công_ty làm giấy của ông Nagao có 6 nhân_công , đặt tại quận Nishi của Nagoya - thành_phố lớn thứ tư và là thành_phố phồn_vinh thứ ba ở Nhật_Bản . Theo báo Mainichi , các công_ty nhỏ ở Nhật chủ_yếu được điều_hành theo kiểu \" cha_truyền_con_nối \" và rất hiếm khi chọn người kế_nhiệm là người ngoài chứ đừng nói đến là người nước_ngoài . Trường_hợp của Công_ty Nagao_Shiko đang được ghi_nhận cho thực_tế_thị_trường lao_động ở Nhật ngày_càng phụ_thuộc vào người lao_động nước_ngoài . Ông Nagao thành_lập công_ty vào năm 1969 sau khi tốt_nghiệp đại_học . Khi đó mong_muốn của ông là làm sống lại doanh_nghiệp mà cha của ông đã đóng_cửa trước đó . Trước_đây , hầu_hết các đơn đặt_hàng đến công_ty là làm loại giấy mỏng cho tã dùng một lần . Các đơn hàng này sau đó giảm mạnh khi các nhà_sản_xuất tả chuyển xưởng ra nước_ngoài cho đỡ tốn phí . Tuy_nhiên , Công_ty Nagao_Shiko đã tận_dụng kỹ_năng kỹ_thuật cao của mình để chuyển_hướng làm_ăn với các nhà_sản_xuất ôtô và công_nghiệp thực_phẩm . Công_ty hiện đang hoạt_động tốt nhờ sản_xuất màng nhiều lớp sử_dụng trong pin xe_hơi và loại màng dùng đóng_gói các phần thực_phẩm bán trong các cửa_hàng tiện_lợi . Ông chủ Nagao bắt_đầu xem_xét tìm người kế_vị khi mình đã hơn 60 tuổi . Con_trai cả của ông làm_việc ở nơi khác và không quan_tâm đến việc tiếp_quản công_ty . Sau khi suy_nghĩ kỹ , ông chọn người sẽ quản_lý công_ty nhỏ của ông là anh Nguyễn_Đức_Trường , 34 tuổi , đến từ Việt_Nam . Anh Trường sang Nhật vào năm 2005 với tư_cách là thực_tập_sinh kỹ_thuật và có giấy_tờ thường_trú sau khi kết_hôn với một phụ_nữ Nhật . Anh vào làm_việc ở công_ty ông Nagao năm 2008 . Thoạt_đầu dù thiếu kinh_nghiệm , nhưng anh Trường học_hỏi nhanh nhờ khéo_léo và ham học . Ông chủ Nhật nhanh_chóng quý_mến , rồi tin_tưởng tuyệt_đối người thợ 34 tuổi của mình bởi anh tận_tuỵ với công_việc , làm được đủ thứ việc từ sửa_chữa máy_móc bị hỏng , đến bất_kỳ hỏng hóc gì khác trong nhà_xưởng . Cách đây vài năm khi được ông chủ Nagao hỏi liệu có muốn tiếp_quản công_ty , anh Trường đã rất ngạc_nhiên và cũng cảm_thấy rất nhiều áp_lực . \" Nhưng tôi cũng rất vui vì ( Nagao ) đã đặt niềm_tin vào tôi rất nhiều và tôi đã quyết_định bảo_vệ công_ty \" , anh Trường kể với báo The_Mainichi . Hiện khoảng 95 % các doanh_nghiệp vừa , nhỏ và siêu nhỏ ở Nhật đang phải đối_mặt với câu_hỏi khẩn_cấp về việc ai sẽ tiếp_quản thế_hệ lãnh_đạo hiện_nay . Theo khảo_sát của Tokyo_Shoko_Research năm 2016-2017 đối_với 4.303 công_ty vừa và 3.984 công_ty nhỏ , có 30,9 % công_ty vừa và 32,4 % công_ty nhỏ thừa_nhận không có ứng_cử_viên kế_nhiệm hoặc chưa quyết_định làm gì với người kế_nhiệm . Khoảng 2,1 % các công_ty vừa và 17,2 % các công_ty nhỏ cho biết thế_hệ lãnh_đạo hiện_tại sẽ là thế_hệ cuối_cùng của các công_ty .  Ông chủ Yasutaka_Nagao ( phải ) chụp ảnh cùng người kế_nhiệm mà ông đã chọn_lựa là anh công_nhân người Việt Nguyễn_Đức_Trường . Hình_ảnh chụp tại xưởng công_ty ở quận Nishi tháng 2-2019 - Ảnh : MAINICHI \n",
            "\n",
            "Prediction:  tn_công hôm 13/3 là cu\n",
            "Truth:  Các đơn_vị không_quân Syria và Nga đã tiến_hành tấn_công mạnh_mẽ nhằm vào cứ_điểm của các phiến quân ở Bắc_Hama và Nam_Idlib để đối_phó với các đòn đánh của nhóm Tahrir al - Sham al - Hay ' at ( hay Al - Nusra ) tại khu_vực phi quân_sự . \n",
            "Content:  Tờ al - Watan hôm 14/3 cho_hay , các máy_bay chiến_đấu Nga và Syria đã tiến_hành các cuộc không_kích nhằm vào nơi ẩn_náu của các tay súng khủng_bố ở al - Tamane ' ah , al - Nayrab , Saraqib , Kafr_Amim và Ma ' arat Hormat ở miền Nam và Đông_Idlib cũng như cứ_điểm của chúng ở Bắc và Tây_Bắc_Hama Các nguồn_tin thực_địa mô_tả vụ tấn_công hôm 13/3 là cuộc không_kích có mức_độ huỷ_diệt lớn nhất của các đơn_vị không_quân Nga và Syria kể từ tháng 9/2018 , khiến số_lượng lớn khủng_bố thiệt_mạng và bị_thương . Cùng với các cuộc không_kích , lực_lượng quân_đội Syria đã ngăn_chặn các cuộc tấn_công của những kẻ khủng_bố từ Kafr_Naboudeh , al - Sakhar và Murak nhằm vào các cứ_điểm quân_sự . Họ cũng tấn_công mục_tiêu di_chuyển của các chiến_binh từ các thị_trấn Ma ' arat , Hormat al - Khowin , al - Zarzour và al - Tamane ' ah về phía các cứ_điểm quân_sự ở Đông_Nam_Idlib bằng các cuộc tấn_công bằng tên_lửa hạng nặng , gây thiệt_hại nặng_nề cho khủng_bố . Các đơn_vị quân_đội Syria cùng lúc đáp trả các cuộc tấn_công của những kẻ khủng_bố vào các khu_vực an_toàn ở Aleppo và Lattakia . Bộ Quốc_phòng Nga cho_biết trong một tuyên_bố rằng Lực_lượng hàng_không_vũ_trụ Nga đã phá_huỷ một kho vũ_khí thuộc nhóm khủng_bố Tahrir al - Sham al - Hay ' at ở tỉnh Idlib của Syria . “ Theo thông_tin được xác_nhận qua một_số nguồn_tin , các chiến_binh trước đó đã chuyển một số_lượng lớn máy_bay_không_người_lái chiến_đấu đến các cứ_điểm , nơi chúng dự_định thực_hiện cuộc tấn_công vào căn_cứ không_quân Hmeimim của Nga ” , thông_cáo của Bộ viết . Một cuộc không_kích chính_xác khác đã được thực_hiện vào ngày 13/3 với sự phối_hợp của Thổ_Nhĩ_Kỳ . Trong một diễn_biến có liên_quan vào ngày 13/3 , quân_đội Syria đã phá_huỷ một trụ sở_chỉ_huy của Tahrir al - Sham ở al - Habit và xung_quanh thị_trấn Babilon . Vụ tấn_công diễn ra khi lực_lượng này đang tiến_hành một cuộc họp giữa các chỉ_huy của tổ_chức , khiến tất_cả những phần_tử có_mặt tại địa_điểm này thiệt_mạng . Một nguồn_tin quân_sự cũng xác_nhận rằng quân_đội Syria đã phá_huỷ một trung_tâm chỉ_huy của Tahrir al - Sham cùng với một_số phương_tiện quân_sự ở thị_trấn Harash_Abedin ở Đông_Nam_Idlib , gây thương_vong nặng_nề cho khủng_bố . Xem thêm : Bất_ngờ câu nói của tay súng trước khi xả đạn điên_cuồng vào nhà_thờ New_Zealand  Ảnh minh_hoạ . \n",
            "\n",
            "Prediction:  trong bà Mnh_Vn_Chu v\n",
            "Truth:  Lần đầu_tiên kể từ khi bị bắt tại Canada tháng 12-2018 , giám_đốc tài_chính toàn_cầu công_ty Huawei , bà Mạnh_Vãn_Chu , gửi thư tới 188.000 nhân_viên công_ty này , cảm_ơn sự động_viên của họ . \n",
            "Content:  Theo đài CNN , trong bản_sao bức thư họ nhận được , bà Mạnh cảm_ơn gần 200.000 nhân_viên Huawei toàn_cầu vì đã hết_lòng ủng_hộ , động_viên bà , trao cho bà \" sức_mạnh \" trong thời_gian qua . Bà Mạnh_Vãn_Chu vẫn đang chờ_đợi phiên xử tại toà Canada để quyết_định việc có bị dẫn_độ qua Mỹ xét_xử không . Chính_quyền Mỹ cáo_buộc bà Mạnh vi_phạm các lệnh trừng_phạt của Mỹ với Iran . \" Cứ mỗi khi phiên xét_xử kết_thúc , tôi lại thấy các nhân_viên của Huawei đứng chờ trắng đêm chỉ để theo_dõi vụ_việc của tôi theo các múi thời_gian cách xa nhau \" , bà Mạnh viết trong bức thư được công_bố trên một diễn_đàn mạng nội_bộ của công_ty Huawei . \" Điều này làm tôi ứa nước_mắt \" . Vụ_việc liên_đới tới bà Mạnh diễn ra trong bối_cảnh công_ty Huawei đối_mặt với nhiều sức_ép từ chính_phủ Mỹ . Chính_quyền tại Washington cáo_buộc các thiết_bị mạng của Huawei tiềm_ẩn nguy_cơ đối_với an_ninh quốc_gia . Không_chỉ thế , Huawei cũng đang đối_mặt với những cáo_buộc tại Seattle cho_rằng công_ty này đã cố_tình đánh_cắp các bí_mật thương_mại của hãng T - Mobile . Tuy_nhiên bà Mạnh và công_ty Huawei cho tới nay vẫn bác_bỏ những cáo_buộc đó . Cũng trong bức thư đầy cảm_xúc , bà Mạnh cho_biết cảm_thấy được động_viên rất nhiều qua những thông_điệp chia_sẻ của mọi người trên diễn_đàn của công_ty , và qua hình_ảnh các cựu nhân_viên của công_ty xếp_hàng bên ngoài trụ_sở toà_án ở thành_phố Vancouver , Canada . Hai tuần sau khi bị bắt , bà Mạnh được tại_ngoại sau khi nộp 7,5 triệu USD tiền bảo_lãnh . Bà cũng đã nộp lại toàn_bộ hộ_chiếu của mình . Hiện bà đang sống tại một trong hai căn nhà thuộc sở_hữu của bà tại thành_phố Vancouver . Bà cũng phải thanh_toán chi_phí canh_gác 24/24 của lực_lượng an_ninh và đeo còng điện_tử có gắn định_vị GPS . \" Bất_kể việc bị hạn_chế về thể_chất trong một không_gian giới_hạn khi sống tại Vancouver , nhưng trong lòng mình , tôi chưa bao_giờ cảm_thấy sự sinh_động và mênh_mông đến thế \" , bà Mạnh viết . Vụ bắt_giữ giám_đốc tài_chính toàn_cầu của Huawei đã dẫn tới những căng_thẳng ngoại_giao giữa Canada , Trung_Quốc và Mỹ thời_gian qua . Nó cũng đã làm phức_tạp thêm tiến_trình đàm_phán thương_mại Mỹ - Trung . Huawei là nhà_sản_xuất thiết_bị viễn_thông lớn nhất thế_giới . Tuần trước , công_ty này cáo_buộc quá_trình xét_xử vấn_đề dẫn_độ bà Mạnh đang có sự chi_phối bởi các yếu_tố chính_trị , theo đó có_thể vi_phạm \" các quyền hợp_pháp \" của bà . Tháng 3 năm nay , bà Mạnh cũng đã khởi_kiện nhà_chức_trách Canada vì đã vi_phạm các quyền của bà ở thời_điểm họ bắt bà năm_ngoái . Dự_kiến bà Mạnh sẽ phải trình_diện trở_lại tại toà Canada trong tháng 9 tới .  Bà Mạnh_Vãn_Chu - Ảnh : AFP \n",
            "\n",
            "Prediction:  ên v ông Nguyên Thanh\n",
            "Truth:  Sau khi bị tổ_chức kỷ luật cảnh cáo , thu hồi 7 lô đất , mới_đây ông Nguyễn Thanh_Sơn , nguyên bí thư đảng ủy khối các cơ_quan tỉnh Đắk Nông , đã làm đơn xin cấp lại \" sổ đỏ \" 7 lô đất đã bị thu_hồi trước đó . \n",
            "Content:  Liên_quan đến vụ \" Chiếm_dụng đất rừng trái_phép , nhiều cán_bộ Đắk_Nông bị kỷ_luật \" , chiều 6-4 , lãnh đạo Phòng Tài nguyên và môi trường huyện Đắk Song ( Đắk Nông ) cho biết ông Nguyễn Thanh_Sơn , nguyên uỷ_viên Ban thường_vụ Tỉnh_uỷ , nguyên bí thư đảng ủy khối các cơ_quan tỉnh Đắk Nông , đã có đơn xin cấp lại \" sổ đỏ \" 7 lô đất đã bị huyện thu hồi trước đó . Trong đơn , ông Sơn nói thừa nhận khuyết điểm về việc nhận đất rừng không đúng đối tượng vì không có hộ khẩu tại địa phương nên đã bị Ủy ban kiểm tra trung_ương ra quyết định thi hành kỷ luật mức cảnh cáo . Hiện nay , ông cũng đã chấp hành nộp 7 giấy chứng nhận quyền sử dụng đất mang tên mình về Phòng Tài nguyên và môi trường huyện Đắk Song theo đúng quy định . Tuy_nhiên , ông Sơn cho rằng các khuyết điểm vi phạm của ông là do \" nghiên cứu không đầy đủ Luật đất đai , chủ quan khi dựa vào sự hướng dẫn của cán bộ xã để làm thủ tục cấp sổ đỏ \" . Cụ thể , vào năm 2003 và 2005 , các cán bộ hướng dẫn ông Sơn làm \" sổ đỏ \" cho bảy lô đất ( tổng diện tích hơn 10,2 ha ) nhưng không nói gì về việc cán bộ công chức không có hộ khẩu thường trú tại địa phương không thuộc trường hợp được cấp đất theo chương trình 135 . Ông Sơn cho rằng tất cả các diện tích này ông và bà Từ Thị Khanh ( vợ ông ) cùng đứng tên nhưng việc canh tác đều do gia đình bên vợ trực tiếp làm để cải thiện cuộc sống . Ông Sơn cho biết vì gia đình ông chú vợ ở Quảng Ngãi quá khó khăn nên vào đây nhờ ông tìm đất sinh sống . Ông đã đến xã Trường Xuân , huyện Đắk Song xin cấp 10 lô đất với tổng diện tích hơn 13ha để gia đình , con_cái người chú họ sinh sống , làm ăn . Ngoài ra , hiện khu đất tại xã Trường Xuân mà ông đã bị thu hồi đang hợp tác thực hiện dự án trung_tâm cây giống cây trồng vật nuôi với một doanh nghiệp rất hiệu quả . \" Để giải quyết có tình , có lý về đất đai , tôi tha thiết đề nghị các anh ( đơn gửi đích danh bí thư huyện ủy , chủ tịch , phó chủ tịch UBND huyện Đắk Song - PV ) xin cấp lại bảy lô đất đã bị thu hồi sổ đỏ vì gia đình tôi đã có hộ khẩu ở đây \" . Trả lời về vấn đề này , ông Đồng Văn Giáp , phó trưởng Phòng Tài nguyên và môi trường huyện Đắk Song , cho biết đã thu hồi 7/10 sổ đỏ của gia đình ông Sơn do vi phạm quy định về đất đai . Tuy_nhiên , đến nay huyện chưa thực hiện việc thu hồi đất vì chưa có hướng giải quyết tài sản gắn liền trên đất của gia đình ông Nguyễn Thanh_Sơn . Hiện ông Sơn cũng có đơn gởi các cấp về việc xin cấp lại \" sổ đỏ \" cho 7 lô đất nêu trên và phòng đã tham_mưu UBND huyện có văn bản xin ý kiến chỉ đạo của tỉnh . \" Thực tế huyện cũng đang gặp vướng mắc trong việc giải quyết tài sản trên đất của hộ gia đình ông Sơn , nên thực tế việc thu hồi chỉ mới \" trên giấy \" . Chúng tôi cũng đang chờ ý kiến cấp trên về việc có \" hợp thức hóa \" sổ đỏ cho gia đình ông Sơn hay không rồi mới thực hiện \" , ông Giáp thông_tin . Trao_đổi với phóng viên Tuổi Trẻ Online , lãnh đạo Huyện ủy Đắk Song khẳng định đã có văn bản gởi cấp trên để xin ý kiến về hướng giải quyết . Tuy_nhiên , quan điểm của huyện là không hợp thức hóa các sai phạm về đất đai cho gia đình ông Sơn .  Bảy sổ đỏ cấp cho ông Sơn , bà Khanh đã bị thu hồi , bấm lỗ và hủy giá trị pháp lý - Ảnh : T R.TÂN . Huyện Đắk Song cho biết chưa thể thu hồi hơn 10ha đất cạnh quốc lộ 14 có nguồn gốc lâm nghiệp mà ông Sơn được cấp sổ đỏ vì chưa có hướng giải quyết tài sản trên đất - Ảnh : T R.TÂN \n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print('\\nPrediction: ',pred_str[i])\n",
        "    print('Truth: ',label_str[i])\n",
        "    print('Content: ',test_data[i]['original'])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n4kLAIQSr5g2",
        "outputId": "b4d745cb-4599-43cf-e8c0-1ec9e5fdba6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'data/vietnews-master/data/test_tokenized/010067.txt.seg'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data[0]['file']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "testing-huggingface",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01644f724526402c9f139e19b2035073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03b57581ace346d2bde4378a7c6309ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09375b2aac814adcbb51589402eb8b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aeb387a0f53469c8ff42e57647831d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7097c890712d418e9dd9c3e33873a2d5",
            "placeholder": "​",
            "style": "IPY_MODEL_01644f724526402c9f139e19b2035073",
            "value": " 6589/6589 [02:35&lt;00:00, 42.40ba/s]"
          }
        },
        "0d6a0b11ec2a4c6d975857678415f505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ef7f4e43319429d9277d55c83cb084d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_825d4b325e9b45859e808cd6b14fdd42",
              "IPY_MODEL_dd0bdf658c6d4c0cb0a091129ecbebb0"
            ],
            "layout": "IPY_MODEL_ab543b6ef5b34ed4acb85fd25304ed75"
          }
        },
        "1ba158d2f09a4c54bc647ebe77d27b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3598a3c015a64c20be134cf4d2e7fbbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5507016090f64bb99dcbf88e14d71eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ac8ad6eea254d369996622911a6a79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e22006901f10483882e1a1f20f645b24",
            "max": 6589,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efd6b87b93244d5ca6817ab35c385510",
            "value": 6589
          }
        },
        "63276058881e45539d8dcd4ff2e05edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "7097c890712d418e9dd9c3e33873a2d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70bb7c0669ca4a3699ad36dfdcc10910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ac8ad6eea254d369996622911a6a79e",
              "IPY_MODEL_0aeb387a0f53469c8ff42e57647831d8"
            ],
            "layout": "IPY_MODEL_1ba158d2f09a4c54bc647ebe77d27b60"
          }
        },
        "825d4b325e9b45859e808cd6b14fdd42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e21d97b40ffa43668b9962c5d771debb",
            "max": 22,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7c296a0b8a84ee3be45c92edb6aad21",
            "value": 22
          }
        },
        "a4898ec38618401396928eee2c4f9926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab543b6ef5b34ed4acb85fd25304ed75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ddde0882d841daa3bfde43ed6e6fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cab0cd369bfa4c80b963ebd27d3e4974",
              "IPY_MODEL_db0264404d934633824b5b381b31a5ac"
            ],
            "layout": "IPY_MODEL_09375b2aac814adcbb51589402eb8b68"
          }
        },
        "cab0cd369bfa4c80b963ebd27d3e4974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03b57581ace346d2bde4378a7c6309ca",
            "max": 1416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63276058881e45539d8dcd4ff2e05edd",
            "value": 1416
          }
        },
        "db0264404d934633824b5b381b31a5ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d6a0b11ec2a4c6d975857678415f505",
            "placeholder": "​",
            "style": "IPY_MODEL_5507016090f64bb99dcbf88e14d71eda",
            "value": " 1416/1416 [00:44&lt;00:00, 31.76ba/s]"
          }
        },
        "dd0bdf658c6d4c0cb0a091129ecbebb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3598a3c015a64c20be134cf4d2e7fbbe",
            "placeholder": "​",
            "style": "IPY_MODEL_a4898ec38618401396928eee2c4f9926",
            "value": " 22/22 [29:59&lt;00:00, 81.81s/ba]"
          }
        },
        "e21d97b40ffa43668b9962c5d771debb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e22006901f10483882e1a1f20f645b24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd6b87b93244d5ca6817ab35c385510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "f7c296a0b8a84ee3be45c92edb6aad21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
